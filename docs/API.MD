# API

These sections detail the external (i.e. CLI, REST or Websocket) as well as the internal (ie Classes/Functions) [API](https://en.wikipedia.org/wiki/Application_programming_interface) of krawler.

## Command-Line Interface

The problem with hooks is that they are configured at application setup time and usually fixed during the whole application lifecycle. It means you would have a to create an application instance for each pipeline youâ€™d like to have, not so simple. This is the reason why krawler can also be used as a command-line utility, where each execution setup the hooks pipeline according to the job to be done. The underlying implementation is managed by the global **run(jobfile, options)** function:
* **jobfile**: a path to a local job file or a jobfile JSON object
* **options**:
  * **proxy**: proxy URL to be used for HTTP requests
  * **proxy-https**: proxy URL to be used for HTTPS requests
  * **user**: user name to be used for proxy
  * **password**: user password to be used for proxy
  * **debug**: debug namespace to output like `krawler*` for all messages or `krawler:hooks*` to only trace hooks messages
  
The options are read from the CLI arguments with the same names. This function is responsible of parsing the job definition including all the required parameters to call the underlying services with the relevant hooks configured (see below). A typical job definition is:
```js
let job = {
  // Options for job executor
  options: {
    workersLimit: 4
  },
  // Store to be used for job output
  store: 'job-store',
  // Common options for all generated tasks
  taskTemplate: {
    // Store to be used for task output
    store: 'job-store',
    id: '<%= jobId %>-<%= taskId %>',
    type: 'xxx',
    options: {
      ...
    }
  },
  // Hooks setup
  hooks: {
    // Tasks service hooks
    tasks: {
      // Hooks to be run after task creation
      after: {
        // Each entry is a hook name and associated options object
        computeSomething: {
          hookOption: ...
        }
      }
    },
    // Jobs service hooks
    jobs: {
      // Hooks to be run before job creation
      before: {
        generateTasks: {
          hookOption: ...
        }
      },
      // Hooks to be run after job creation
      after: {
        generateOutput: {
          hookOption: ...
        }
      }
    }
  },
  // The list of tasks to run if not generated by hooks
  tasks: [
  ...
  ]
}
```

> Because hook names are managed as a JSON object keys you cannot have the same hook appearing twice in your pipeline, however you can do that using the parallel syntax (see below)

By default all hooks are run in sequence, if at given step of your pipeline you want a parallel execution of some you can use the reserved hook name `parallel` and give the hooks to be run in parallel as an array of items each containing the hook name as a `hook` property and its options as other properties:
```js
tasks: {
  after: {
    readXML: {
    },
    parallel: [
      {
        hook: 'writeTemplate',
        templateFile: 'mapproxy-template.yaml'
      },
      {
        hook: 'writeTemplate',
        templateFile: 'leaflet-template.html'
      }
    ]
  }
}
```

## Services

[Stores](./SERVICES.MD#stores) to manage storage backends

[Tasks](./SERVICES.MD#tasks) to manage task execution

[Jobs](./SERVICES.MD#jobs) to manage job request execution

[Complete Example](./SERVICES.MD#complete-example)

## Hooks

[Authentication](./HOOKS.MD#authentication-source) management

[CSV](./HOOKS.MD#csv-source) file management

[JSON](./HOOKS.MD#json-source) file and structures management

[XML](./HOOKS.MD#xml-source) file management

[YAML](./HOOKS.MD#yaml-source) file management

[Raster](./HOOKS.MD#raster-source) file management

[PostgreSQL](./HOOKS.MD#postgresql-source) database management

[Store](./HOOKS.MD#store-source) management

[Clearing](./HOOKS.MD#clearing-source) of output files management
